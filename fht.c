#include "fht.h"

inline void helper_1(float *buf) {
  for (int j = 0; j < 2; j += 2) {
    for (int k = 0; k < 1; ++k) {
      float u = buf[j + k];
      float v = buf[j + k + 1];
      buf[j + k] = u + v;
      buf[j + k + 1] = u - v;
    }
  }
}

inline void helper_2(float *buf) {
  for (int j = 0; j < 4; j += 2) {
    for (int k = 0; k < 1; ++k) {
      float u = buf[j + k];
      float v = buf[j + k + 1];
      buf[j + k] = u + v;
      buf[j + k + 1] = u - v;
    }
  }
  for (int j = 0; j < 4; j += 4) {
    for (int k = 0; k < 2; ++k) {
      float u = buf[j + k];
      float v = buf[j + k + 2];
      buf[j + k] = u + v;
      buf[j + k + 2] = u - v;
    }
  }
}

inline void helper_3(float *buf);
inline void helper_3(float *buf) {
  for (int j = 0; j < 8; j += 8) {
    __asm__ volatile (
      "vmovups (%0), %%ymm0\n"
      "vpermilps $160, %%ymm0, %%ymm8\n"
      "vpermilps $245, %%ymm0, %%ymm9\n"
      "vxorps %%ymm10, %%ymm10, %%ymm10\n"
      "vsubps %%ymm9, %%ymm10, %%ymm11\n"
      "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
      "vpermilps $68, %%ymm0, %%ymm8\n"
      "vpermilps $238, %%ymm0, %%ymm9\n"
      "vxorps %%ymm10, %%ymm10, %%ymm10\n"
      "vsubps %%ymm9, %%ymm10, %%ymm11\n"
      "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
      "vaddps %%ymm8, %%ymm12, %%ymm0\n"
      "vxorps %%ymm8, %%ymm8, %%ymm8\n"
      "vsubps %%ymm0, %%ymm8, %%ymm9\n"
      "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
      "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
      "vaddps %%ymm10, %%ymm11, %%ymm0\n"
      "vmovups %%ymm0, (%0)\n"
      :: "r"(buf + j) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
    );
  }
}

inline void helper_4(float *buf);
inline void helper_4(float *buf) {
  for (int j = 0; j < 16; j += 16) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $160, %%ymm1, %%ymm8\n"
        "vpermilps $245, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vpermilps $68, %%ymm1, %%ymm8\n"
        "vpermilps $238, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm1\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm1, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm1\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 8) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void helper_5(float *buf);
inline void helper_5(float *buf) {
  for (int j = 0; j < 32; j += 32) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $160, %%ymm1, %%ymm8\n"
        "vpermilps $245, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilps $160, %%ymm2, %%ymm8\n"
        "vpermilps $245, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilps $160, %%ymm3, %%ymm8\n"
        "vpermilps $245, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vpermilps $68, %%ymm1, %%ymm8\n"
        "vpermilps $238, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm1\n"
        "vpermilps $68, %%ymm2, %%ymm8\n"
        "vpermilps $238, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm2\n"
        "vpermilps $68, %%ymm3, %%ymm8\n"
        "vpermilps $238, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm3\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm1, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm1\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm2, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm2\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm3, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm3\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vmovups %%ymm0, (%0)\n"
        "vmovups %%ymm1, (%1)\n"
        "vmovups %%ymm2, (%2)\n"
        "vmovups %%ymm3, (%3)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void helper_6(float *buf);
inline void helper_6(float *buf) {
  for (int j = 0; j < 64; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $160, %%ymm1, %%ymm8\n"
        "vpermilps $245, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilps $160, %%ymm2, %%ymm8\n"
        "vpermilps $245, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilps $160, %%ymm3, %%ymm8\n"
        "vpermilps $245, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilps $160, %%ymm4, %%ymm8\n"
        "vpermilps $245, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilps $160, %%ymm5, %%ymm8\n"
        "vpermilps $245, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilps $160, %%ymm6, %%ymm8\n"
        "vpermilps $245, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilps $160, %%ymm7, %%ymm8\n"
        "vpermilps $245, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vpermilps $68, %%ymm1, %%ymm8\n"
        "vpermilps $238, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm1\n"
        "vpermilps $68, %%ymm2, %%ymm8\n"
        "vpermilps $238, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm2\n"
        "vpermilps $68, %%ymm3, %%ymm8\n"
        "vpermilps $238, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm3\n"
        "vpermilps $68, %%ymm4, %%ymm8\n"
        "vpermilps $238, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm4\n"
        "vpermilps $68, %%ymm5, %%ymm8\n"
        "vpermilps $238, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm5\n"
        "vpermilps $68, %%ymm6, %%ymm8\n"
        "vpermilps $238, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm6\n"
        "vpermilps $68, %%ymm7, %%ymm8\n"
        "vpermilps $238, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm7\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm1, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm1\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm2, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm2\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm3, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm3\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm4, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm4\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm5, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm5\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm6, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm6\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm7, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void helper_7(float *buf);
inline void helper_7(float *buf) {
  for (int j = 0; j < 128; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $160, %%ymm1, %%ymm8\n"
        "vpermilps $245, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilps $160, %%ymm2, %%ymm8\n"
        "vpermilps $245, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilps $160, %%ymm3, %%ymm8\n"
        "vpermilps $245, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilps $160, %%ymm4, %%ymm8\n"
        "vpermilps $245, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilps $160, %%ymm5, %%ymm8\n"
        "vpermilps $245, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilps $160, %%ymm6, %%ymm8\n"
        "vpermilps $245, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilps $160, %%ymm7, %%ymm8\n"
        "vpermilps $245, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vpermilps $68, %%ymm1, %%ymm8\n"
        "vpermilps $238, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm1\n"
        "vpermilps $68, %%ymm2, %%ymm8\n"
        "vpermilps $238, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm2\n"
        "vpermilps $68, %%ymm3, %%ymm8\n"
        "vpermilps $238, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm3\n"
        "vpermilps $68, %%ymm4, %%ymm8\n"
        "vpermilps $238, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm4\n"
        "vpermilps $68, %%ymm5, %%ymm8\n"
        "vpermilps $238, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm5\n"
        "vpermilps $68, %%ymm6, %%ymm8\n"
        "vpermilps $238, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm6\n"
        "vpermilps $68, %%ymm7, %%ymm8\n"
        "vpermilps $238, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm7\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm1, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm1\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm2, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm2\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm3, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm3\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm4, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm4\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm5, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm5\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm6, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm6\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm7, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 128; j += 128) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 64) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void helper_8(float *buf);
inline void helper_8(float *buf) {
  for (int j = 0; j < 256; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $160, %%ymm1, %%ymm8\n"
        "vpermilps $245, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilps $160, %%ymm2, %%ymm8\n"
        "vpermilps $245, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilps $160, %%ymm3, %%ymm8\n"
        "vpermilps $245, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilps $160, %%ymm4, %%ymm8\n"
        "vpermilps $245, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilps $160, %%ymm5, %%ymm8\n"
        "vpermilps $245, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilps $160, %%ymm6, %%ymm8\n"
        "vpermilps $245, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilps $160, %%ymm7, %%ymm8\n"
        "vpermilps $245, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vpermilps $68, %%ymm1, %%ymm8\n"
        "vpermilps $238, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm1\n"
        "vpermilps $68, %%ymm2, %%ymm8\n"
        "vpermilps $238, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm2\n"
        "vpermilps $68, %%ymm3, %%ymm8\n"
        "vpermilps $238, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm3\n"
        "vpermilps $68, %%ymm4, %%ymm8\n"
        "vpermilps $238, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm4\n"
        "vpermilps $68, %%ymm5, %%ymm8\n"
        "vpermilps $238, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm5\n"
        "vpermilps $68, %%ymm6, %%ymm8\n"
        "vpermilps $238, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm6\n"
        "vpermilps $68, %%ymm7, %%ymm8\n"
        "vpermilps $238, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm7\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm1, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm1\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm2, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm2\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm3, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm3\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm4, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm4\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm5, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm5\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm6, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm6\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm7, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 256; j += 256) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vmovups %%ymm0, (%0)\n"
        "vmovups %%ymm1, (%1)\n"
        "vmovups %%ymm2, (%2)\n"
        "vmovups %%ymm3, (%3)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void helper_9(float *buf);
inline void helper_9(float *buf) {
  for (int j = 0; j < 512; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $160, %%ymm1, %%ymm8\n"
        "vpermilps $245, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilps $160, %%ymm2, %%ymm8\n"
        "vpermilps $245, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilps $160, %%ymm3, %%ymm8\n"
        "vpermilps $245, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilps $160, %%ymm4, %%ymm8\n"
        "vpermilps $245, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilps $160, %%ymm5, %%ymm8\n"
        "vpermilps $245, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilps $160, %%ymm6, %%ymm8\n"
        "vpermilps $245, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilps $160, %%ymm7, %%ymm8\n"
        "vpermilps $245, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vpermilps $68, %%ymm1, %%ymm8\n"
        "vpermilps $238, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm1\n"
        "vpermilps $68, %%ymm2, %%ymm8\n"
        "vpermilps $238, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm2\n"
        "vpermilps $68, %%ymm3, %%ymm8\n"
        "vpermilps $238, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm3\n"
        "vpermilps $68, %%ymm4, %%ymm8\n"
        "vpermilps $238, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm4\n"
        "vpermilps $68, %%ymm5, %%ymm8\n"
        "vpermilps $238, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm5\n"
        "vpermilps $68, %%ymm6, %%ymm8\n"
        "vpermilps $238, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm6\n"
        "vpermilps $68, %%ymm7, %%ymm8\n"
        "vpermilps $238, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm7\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm1, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm1\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm2, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm2\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm3, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm3\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm4, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm4\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm5, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm5\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm6, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm6\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm7, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 512; j += 512) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void helper_10(float *buf);
inline void helper_10(float *buf) {
  for (int j = 0; j < 1024; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $160, %%ymm1, %%ymm8\n"
        "vpermilps $245, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilps $160, %%ymm2, %%ymm8\n"
        "vpermilps $245, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilps $160, %%ymm3, %%ymm8\n"
        "vpermilps $245, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilps $160, %%ymm4, %%ymm8\n"
        "vpermilps $245, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilps $160, %%ymm5, %%ymm8\n"
        "vpermilps $245, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilps $160, %%ymm6, %%ymm8\n"
        "vpermilps $245, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilps $160, %%ymm7, %%ymm8\n"
        "vpermilps $245, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vpermilps $68, %%ymm1, %%ymm8\n"
        "vpermilps $238, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm1\n"
        "vpermilps $68, %%ymm2, %%ymm8\n"
        "vpermilps $238, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm2\n"
        "vpermilps $68, %%ymm3, %%ymm8\n"
        "vpermilps $238, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm3\n"
        "vpermilps $68, %%ymm4, %%ymm8\n"
        "vpermilps $238, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm4\n"
        "vpermilps $68, %%ymm5, %%ymm8\n"
        "vpermilps $238, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm5\n"
        "vpermilps $68, %%ymm6, %%ymm8\n"
        "vpermilps $238, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm6\n"
        "vpermilps $68, %%ymm7, %%ymm8\n"
        "vpermilps $238, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm7\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm1, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm1\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm2, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm2\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm3, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm3\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm4, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm4\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm5, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm5\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm6, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm6\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm7, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 1024; j += 512) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 1024; j += 1024) {
    for (int k = 0; k < 512; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 512) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void helper_11(float *buf);
inline void helper_11(float *buf) {
  for (int j = 0; j < 2048; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $160, %%ymm1, %%ymm8\n"
        "vpermilps $245, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilps $160, %%ymm2, %%ymm8\n"
        "vpermilps $245, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilps $160, %%ymm3, %%ymm8\n"
        "vpermilps $245, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilps $160, %%ymm4, %%ymm8\n"
        "vpermilps $245, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilps $160, %%ymm5, %%ymm8\n"
        "vpermilps $245, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilps $160, %%ymm6, %%ymm8\n"
        "vpermilps $245, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilps $160, %%ymm7, %%ymm8\n"
        "vpermilps $245, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vpermilps $68, %%ymm1, %%ymm8\n"
        "vpermilps $238, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm1\n"
        "vpermilps $68, %%ymm2, %%ymm8\n"
        "vpermilps $238, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm2\n"
        "vpermilps $68, %%ymm3, %%ymm8\n"
        "vpermilps $238, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm3\n"
        "vpermilps $68, %%ymm4, %%ymm8\n"
        "vpermilps $238, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm4\n"
        "vpermilps $68, %%ymm5, %%ymm8\n"
        "vpermilps $238, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm5\n"
        "vpermilps $68, %%ymm6, %%ymm8\n"
        "vpermilps $238, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm6\n"
        "vpermilps $68, %%ymm7, %%ymm8\n"
        "vpermilps $238, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm7\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm1, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm1\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm2, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm2\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm3, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm3\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm4, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm4\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm5, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm5\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm6, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm6\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm7, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 2048; j += 512) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 2048; j += 2048) {
    for (int k = 0; k < 512; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vmovups %%ymm0, (%0)\n"
        "vmovups %%ymm1, (%1)\n"
        "vmovups %%ymm2, (%2)\n"
        "vmovups %%ymm3, (%3)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void helper_12(float *buf);
inline void helper_12(float *buf) {
  for (int j = 0; j < 4096; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $160, %%ymm1, %%ymm8\n"
        "vpermilps $245, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilps $160, %%ymm2, %%ymm8\n"
        "vpermilps $245, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilps $160, %%ymm3, %%ymm8\n"
        "vpermilps $245, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilps $160, %%ymm4, %%ymm8\n"
        "vpermilps $245, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilps $160, %%ymm5, %%ymm8\n"
        "vpermilps $245, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilps $160, %%ymm6, %%ymm8\n"
        "vpermilps $245, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilps $160, %%ymm7, %%ymm8\n"
        "vpermilps $245, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vpermilps $68, %%ymm1, %%ymm8\n"
        "vpermilps $238, %%ymm1, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm1\n"
        "vpermilps $68, %%ymm2, %%ymm8\n"
        "vpermilps $238, %%ymm2, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm2\n"
        "vpermilps $68, %%ymm3, %%ymm8\n"
        "vpermilps $238, %%ymm3, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm3\n"
        "vpermilps $68, %%ymm4, %%ymm8\n"
        "vpermilps $238, %%ymm4, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm4\n"
        "vpermilps $68, %%ymm5, %%ymm8\n"
        "vpermilps $238, %%ymm5, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm5\n"
        "vpermilps $68, %%ymm6, %%ymm8\n"
        "vpermilps $238, %%ymm6, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm6\n"
        "vpermilps $68, %%ymm7, %%ymm8\n"
        "vpermilps $238, %%ymm7, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm7\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm1, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm1\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm2, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm2\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm3, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm3\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm4, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm4\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm5, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm5\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm6, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm6\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm7, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 4096; j += 512) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 4096; j += 4096) {
    for (int k = 0; k < 512; k += 8) {
      __asm__ volatile (
        "vmovups (%0), %%ymm0\n"
        "vmovups (%1), %%ymm1\n"
        "vmovups (%2), %%ymm2\n"
        "vmovups (%3), %%ymm3\n"
        "vmovups (%4), %%ymm4\n"
        "vmovups (%5), %%ymm5\n"
        "vmovups (%6), %%ymm6\n"
        "vmovups (%7), %%ymm7\n"
        "vaddps %%ymm1, %%ymm0, %%ymm8\n"
        "vsubps %%ymm1, %%ymm0, %%ymm9\n"
        "vaddps %%ymm3, %%ymm2, %%ymm10\n"
        "vsubps %%ymm3, %%ymm2, %%ymm11\n"
        "vaddps %%ymm5, %%ymm4, %%ymm12\n"
        "vsubps %%ymm5, %%ymm4, %%ymm13\n"
        "vaddps %%ymm7, %%ymm6, %%ymm14\n"
        "vsubps %%ymm7, %%ymm6, %%ymm15\n"
        "vaddps %%ymm10, %%ymm8, %%ymm0\n"
        "vsubps %%ymm10, %%ymm8, %%ymm2\n"
        "vaddps %%ymm11, %%ymm9, %%ymm1\n"
        "vsubps %%ymm11, %%ymm9, %%ymm3\n"
        "vaddps %%ymm14, %%ymm12, %%ymm4\n"
        "vsubps %%ymm14, %%ymm12, %%ymm6\n"
        "vaddps %%ymm15, %%ymm13, %%ymm5\n"
        "vsubps %%ymm15, %%ymm13, %%ymm7\n"
        "vaddps %%ymm4, %%ymm0, %%ymm8\n"
        "vsubps %%ymm4, %%ymm0, %%ymm12\n"
        "vaddps %%ymm5, %%ymm1, %%ymm9\n"
        "vsubps %%ymm5, %%ymm1, %%ymm13\n"
        "vaddps %%ymm6, %%ymm2, %%ymm10\n"
        "vsubps %%ymm6, %%ymm2, %%ymm14\n"
        "vaddps %%ymm7, %%ymm3, %%ymm11\n"
        "vsubps %%ymm7, %%ymm3, %%ymm15\n"
        "vmovups %%ymm8, (%0)\n"
        "vmovups %%ymm9, (%1)\n"
        "vmovups %%ymm10, (%2)\n"
        "vmovups %%ymm11, (%3)\n"
        "vmovups %%ymm12, (%4)\n"
        "vmovups %%ymm13, (%5)\n"
        "vmovups %%ymm14, (%6)\n"
        "vmovups %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

void helper_13(float *buf, int depth);
void helper_13(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 13) {
    helper_13(buf + 0, 12);
    helper_13(buf + 4096, 12);
    for (int j = 0; j < 8192; j += 8192) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_14(float *buf, int depth);
void helper_14(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    helper_14(buf + 0, 12);
    helper_14(buf + 4096, 12);
    helper_14(buf + 8192, 12);
    helper_14(buf + 12288, 12);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vmovups %%ymm0, (%0)\n"
          "vmovups %%ymm1, (%1)\n"
          "vmovups %%ymm2, (%2)\n"
          "vmovups %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_15(float *buf, int depth);
void helper_15(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_15(buf + 0, 12);
    helper_15(buf + 4096, 12);
    helper_15(buf + 8192, 12);
    helper_15(buf + 12288, 12);
    helper_15(buf + 16384, 12);
    helper_15(buf + 20480, 12);
    helper_15(buf + 24576, 12);
    helper_15(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_16(float *buf, int depth);
void helper_16(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_16(buf + 0, 12);
    helper_16(buf + 4096, 12);
    helper_16(buf + 8192, 12);
    helper_16(buf + 12288, 12);
    helper_16(buf + 16384, 12);
    helper_16(buf + 20480, 12);
    helper_16(buf + 24576, 12);
    helper_16(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 16) {
    helper_16(buf + 0, 15);
    helper_16(buf + 32768, 15);
    for (int j = 0; j < 65536; j += 65536) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_17(float *buf, int depth);
void helper_17(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_17(buf + 0, 12);
    helper_17(buf + 4096, 12);
    helper_17(buf + 8192, 12);
    helper_17(buf + 12288, 12);
    helper_17(buf + 16384, 12);
    helper_17(buf + 20480, 12);
    helper_17(buf + 24576, 12);
    helper_17(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    helper_17(buf + 0, 15);
    helper_17(buf + 32768, 15);
    helper_17(buf + 65536, 15);
    helper_17(buf + 98304, 15);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vmovups %%ymm0, (%0)\n"
          "vmovups %%ymm1, (%1)\n"
          "vmovups %%ymm2, (%2)\n"
          "vmovups %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_18(float *buf, int depth);
void helper_18(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_18(buf + 0, 12);
    helper_18(buf + 4096, 12);
    helper_18(buf + 8192, 12);
    helper_18(buf + 12288, 12);
    helper_18(buf + 16384, 12);
    helper_18(buf + 20480, 12);
    helper_18(buf + 24576, 12);
    helper_18(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_18(buf + 0, 15);
    helper_18(buf + 32768, 15);
    helper_18(buf + 65536, 15);
    helper_18(buf + 98304, 15);
    helper_18(buf + 131072, 15);
    helper_18(buf + 163840, 15);
    helper_18(buf + 196608, 15);
    helper_18(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_19(float *buf, int depth);
void helper_19(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_19(buf + 0, 12);
    helper_19(buf + 4096, 12);
    helper_19(buf + 8192, 12);
    helper_19(buf + 12288, 12);
    helper_19(buf + 16384, 12);
    helper_19(buf + 20480, 12);
    helper_19(buf + 24576, 12);
    helper_19(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_19(buf + 0, 15);
    helper_19(buf + 32768, 15);
    helper_19(buf + 65536, 15);
    helper_19(buf + 98304, 15);
    helper_19(buf + 131072, 15);
    helper_19(buf + 163840, 15);
    helper_19(buf + 196608, 15);
    helper_19(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 19) {
    helper_19(buf + 0, 18);
    helper_19(buf + 262144, 18);
    for (int j = 0; j < 524288; j += 524288) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_20(float *buf, int depth);
void helper_20(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_20(buf + 0, 12);
    helper_20(buf + 4096, 12);
    helper_20(buf + 8192, 12);
    helper_20(buf + 12288, 12);
    helper_20(buf + 16384, 12);
    helper_20(buf + 20480, 12);
    helper_20(buf + 24576, 12);
    helper_20(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_20(buf + 0, 15);
    helper_20(buf + 32768, 15);
    helper_20(buf + 65536, 15);
    helper_20(buf + 98304, 15);
    helper_20(buf + 131072, 15);
    helper_20(buf + 163840, 15);
    helper_20(buf + 196608, 15);
    helper_20(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    helper_20(buf + 0, 18);
    helper_20(buf + 262144, 18);
    helper_20(buf + 524288, 18);
    helper_20(buf + 786432, 18);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vmovups %%ymm0, (%0)\n"
          "vmovups %%ymm1, (%1)\n"
          "vmovups %%ymm2, (%2)\n"
          "vmovups %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_21(float *buf, int depth);
void helper_21(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_21(buf + 0, 12);
    helper_21(buf + 4096, 12);
    helper_21(buf + 8192, 12);
    helper_21(buf + 12288, 12);
    helper_21(buf + 16384, 12);
    helper_21(buf + 20480, 12);
    helper_21(buf + 24576, 12);
    helper_21(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_21(buf + 0, 15);
    helper_21(buf + 32768, 15);
    helper_21(buf + 65536, 15);
    helper_21(buf + 98304, 15);
    helper_21(buf + 131072, 15);
    helper_21(buf + 163840, 15);
    helper_21(buf + 196608, 15);
    helper_21(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_21(buf + 0, 18);
    helper_21(buf + 262144, 18);
    helper_21(buf + 524288, 18);
    helper_21(buf + 786432, 18);
    helper_21(buf + 1048576, 18);
    helper_21(buf + 1310720, 18);
    helper_21(buf + 1572864, 18);
    helper_21(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_22(float *buf, int depth);
void helper_22(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_22(buf + 0, 12);
    helper_22(buf + 4096, 12);
    helper_22(buf + 8192, 12);
    helper_22(buf + 12288, 12);
    helper_22(buf + 16384, 12);
    helper_22(buf + 20480, 12);
    helper_22(buf + 24576, 12);
    helper_22(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_22(buf + 0, 15);
    helper_22(buf + 32768, 15);
    helper_22(buf + 65536, 15);
    helper_22(buf + 98304, 15);
    helper_22(buf + 131072, 15);
    helper_22(buf + 163840, 15);
    helper_22(buf + 196608, 15);
    helper_22(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_22(buf + 0, 18);
    helper_22(buf + 262144, 18);
    helper_22(buf + 524288, 18);
    helper_22(buf + 786432, 18);
    helper_22(buf + 1048576, 18);
    helper_22(buf + 1310720, 18);
    helper_22(buf + 1572864, 18);
    helper_22(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 22) {
    helper_22(buf + 0, 21);
    helper_22(buf + 2097152, 21);
    for (int j = 0; j < 4194304; j += 4194304) {
      for (int k = 0; k < 2097152; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2097152) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_23(float *buf, int depth);
void helper_23(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_23(buf + 0, 12);
    helper_23(buf + 4096, 12);
    helper_23(buf + 8192, 12);
    helper_23(buf + 12288, 12);
    helper_23(buf + 16384, 12);
    helper_23(buf + 20480, 12);
    helper_23(buf + 24576, 12);
    helper_23(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_23(buf + 0, 15);
    helper_23(buf + 32768, 15);
    helper_23(buf + 65536, 15);
    helper_23(buf + 98304, 15);
    helper_23(buf + 131072, 15);
    helper_23(buf + 163840, 15);
    helper_23(buf + 196608, 15);
    helper_23(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_23(buf + 0, 18);
    helper_23(buf + 262144, 18);
    helper_23(buf + 524288, 18);
    helper_23(buf + 786432, 18);
    helper_23(buf + 1048576, 18);
    helper_23(buf + 1310720, 18);
    helper_23(buf + 1572864, 18);
    helper_23(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 23) {
    helper_23(buf + 0, 21);
    helper_23(buf + 2097152, 21);
    helper_23(buf + 4194304, 21);
    helper_23(buf + 6291456, 21);
    for (int j = 0; j < 8388608; j += 8388608) {
      for (int k = 0; k < 2097152; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vmovups %%ymm0, (%0)\n"
          "vmovups %%ymm1, (%1)\n"
          "vmovups %%ymm2, (%2)\n"
          "vmovups %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2097152), "r"(buf + j + k + 4194304), "r"(buf + j + k + 6291456) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_24(float *buf, int depth);
void helper_24(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_24(buf + 0, 12);
    helper_24(buf + 4096, 12);
    helper_24(buf + 8192, 12);
    helper_24(buf + 12288, 12);
    helper_24(buf + 16384, 12);
    helper_24(buf + 20480, 12);
    helper_24(buf + 24576, 12);
    helper_24(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_24(buf + 0, 15);
    helper_24(buf + 32768, 15);
    helper_24(buf + 65536, 15);
    helper_24(buf + 98304, 15);
    helper_24(buf + 131072, 15);
    helper_24(buf + 163840, 15);
    helper_24(buf + 196608, 15);
    helper_24(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_24(buf + 0, 18);
    helper_24(buf + 262144, 18);
    helper_24(buf + 524288, 18);
    helper_24(buf + 786432, 18);
    helper_24(buf + 1048576, 18);
    helper_24(buf + 1310720, 18);
    helper_24(buf + 1572864, 18);
    helper_24(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 24) {
    helper_24(buf + 0, 21);
    helper_24(buf + 2097152, 21);
    helper_24(buf + 4194304, 21);
    helper_24(buf + 6291456, 21);
    helper_24(buf + 8388608, 21);
    helper_24(buf + 10485760, 21);
    helper_24(buf + 12582912, 21);
    helper_24(buf + 14680064, 21);
    for (int j = 0; j < 16777216; j += 16777216) {
      for (int k = 0; k < 2097152; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2097152), "r"(buf + j + k + 4194304), "r"(buf + j + k + 6291456), "r"(buf + j + k + 8388608), "r"(buf + j + k + 10485760), "r"(buf + j + k + 12582912), "r"(buf + j + k + 14680064) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_25(float *buf, int depth);
void helper_25(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_25(buf + 0, 12);
    helper_25(buf + 4096, 12);
    helper_25(buf + 8192, 12);
    helper_25(buf + 12288, 12);
    helper_25(buf + 16384, 12);
    helper_25(buf + 20480, 12);
    helper_25(buf + 24576, 12);
    helper_25(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_25(buf + 0, 15);
    helper_25(buf + 32768, 15);
    helper_25(buf + 65536, 15);
    helper_25(buf + 98304, 15);
    helper_25(buf + 131072, 15);
    helper_25(buf + 163840, 15);
    helper_25(buf + 196608, 15);
    helper_25(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_25(buf + 0, 18);
    helper_25(buf + 262144, 18);
    helper_25(buf + 524288, 18);
    helper_25(buf + 786432, 18);
    helper_25(buf + 1048576, 18);
    helper_25(buf + 1310720, 18);
    helper_25(buf + 1572864, 18);
    helper_25(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 24) {
    helper_25(buf + 0, 21);
    helper_25(buf + 2097152, 21);
    helper_25(buf + 4194304, 21);
    helper_25(buf + 6291456, 21);
    helper_25(buf + 8388608, 21);
    helper_25(buf + 10485760, 21);
    helper_25(buf + 12582912, 21);
    helper_25(buf + 14680064, 21);
    for (int j = 0; j < 16777216; j += 16777216) {
      for (int k = 0; k < 2097152; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2097152), "r"(buf + j + k + 4194304), "r"(buf + j + k + 6291456), "r"(buf + j + k + 8388608), "r"(buf + j + k + 10485760), "r"(buf + j + k + 12582912), "r"(buf + j + k + 14680064) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 25) {
    helper_25(buf + 0, 24);
    helper_25(buf + 16777216, 24);
    for (int j = 0; j < 33554432; j += 33554432) {
      for (int k = 0; k < 16777216; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16777216) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_26(float *buf, int depth);
void helper_26(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_26(buf + 0, 12);
    helper_26(buf + 4096, 12);
    helper_26(buf + 8192, 12);
    helper_26(buf + 12288, 12);
    helper_26(buf + 16384, 12);
    helper_26(buf + 20480, 12);
    helper_26(buf + 24576, 12);
    helper_26(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_26(buf + 0, 15);
    helper_26(buf + 32768, 15);
    helper_26(buf + 65536, 15);
    helper_26(buf + 98304, 15);
    helper_26(buf + 131072, 15);
    helper_26(buf + 163840, 15);
    helper_26(buf + 196608, 15);
    helper_26(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_26(buf + 0, 18);
    helper_26(buf + 262144, 18);
    helper_26(buf + 524288, 18);
    helper_26(buf + 786432, 18);
    helper_26(buf + 1048576, 18);
    helper_26(buf + 1310720, 18);
    helper_26(buf + 1572864, 18);
    helper_26(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 24) {
    helper_26(buf + 0, 21);
    helper_26(buf + 2097152, 21);
    helper_26(buf + 4194304, 21);
    helper_26(buf + 6291456, 21);
    helper_26(buf + 8388608, 21);
    helper_26(buf + 10485760, 21);
    helper_26(buf + 12582912, 21);
    helper_26(buf + 14680064, 21);
    for (int j = 0; j < 16777216; j += 16777216) {
      for (int k = 0; k < 2097152; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2097152), "r"(buf + j + k + 4194304), "r"(buf + j + k + 6291456), "r"(buf + j + k + 8388608), "r"(buf + j + k + 10485760), "r"(buf + j + k + 12582912), "r"(buf + j + k + 14680064) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 26) {
    helper_26(buf + 0, 24);
    helper_26(buf + 16777216, 24);
    helper_26(buf + 33554432, 24);
    helper_26(buf + 50331648, 24);
    for (int j = 0; j < 67108864; j += 67108864) {
      for (int k = 0; k < 16777216; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vmovups %%ymm0, (%0)\n"
          "vmovups %%ymm1, (%1)\n"
          "vmovups %%ymm2, (%2)\n"
          "vmovups %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16777216), "r"(buf + j + k + 33554432), "r"(buf + j + k + 50331648) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_27(float *buf, int depth);
void helper_27(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_27(buf + 0, 12);
    helper_27(buf + 4096, 12);
    helper_27(buf + 8192, 12);
    helper_27(buf + 12288, 12);
    helper_27(buf + 16384, 12);
    helper_27(buf + 20480, 12);
    helper_27(buf + 24576, 12);
    helper_27(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_27(buf + 0, 15);
    helper_27(buf + 32768, 15);
    helper_27(buf + 65536, 15);
    helper_27(buf + 98304, 15);
    helper_27(buf + 131072, 15);
    helper_27(buf + 163840, 15);
    helper_27(buf + 196608, 15);
    helper_27(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_27(buf + 0, 18);
    helper_27(buf + 262144, 18);
    helper_27(buf + 524288, 18);
    helper_27(buf + 786432, 18);
    helper_27(buf + 1048576, 18);
    helper_27(buf + 1310720, 18);
    helper_27(buf + 1572864, 18);
    helper_27(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 24) {
    helper_27(buf + 0, 21);
    helper_27(buf + 2097152, 21);
    helper_27(buf + 4194304, 21);
    helper_27(buf + 6291456, 21);
    helper_27(buf + 8388608, 21);
    helper_27(buf + 10485760, 21);
    helper_27(buf + 12582912, 21);
    helper_27(buf + 14680064, 21);
    for (int j = 0; j < 16777216; j += 16777216) {
      for (int k = 0; k < 2097152; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2097152), "r"(buf + j + k + 4194304), "r"(buf + j + k + 6291456), "r"(buf + j + k + 8388608), "r"(buf + j + k + 10485760), "r"(buf + j + k + 12582912), "r"(buf + j + k + 14680064) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 27) {
    helper_27(buf + 0, 24);
    helper_27(buf + 16777216, 24);
    helper_27(buf + 33554432, 24);
    helper_27(buf + 50331648, 24);
    helper_27(buf + 67108864, 24);
    helper_27(buf + 83886080, 24);
    helper_27(buf + 100663296, 24);
    helper_27(buf + 117440512, 24);
    for (int j = 0; j < 134217728; j += 134217728) {
      for (int k = 0; k < 16777216; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16777216), "r"(buf + j + k + 33554432), "r"(buf + j + k + 50331648), "r"(buf + j + k + 67108864), "r"(buf + j + k + 83886080), "r"(buf + j + k + 100663296), "r"(buf + j + k + 117440512) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_28(float *buf, int depth);
void helper_28(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_28(buf + 0, 12);
    helper_28(buf + 4096, 12);
    helper_28(buf + 8192, 12);
    helper_28(buf + 12288, 12);
    helper_28(buf + 16384, 12);
    helper_28(buf + 20480, 12);
    helper_28(buf + 24576, 12);
    helper_28(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_28(buf + 0, 15);
    helper_28(buf + 32768, 15);
    helper_28(buf + 65536, 15);
    helper_28(buf + 98304, 15);
    helper_28(buf + 131072, 15);
    helper_28(buf + 163840, 15);
    helper_28(buf + 196608, 15);
    helper_28(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_28(buf + 0, 18);
    helper_28(buf + 262144, 18);
    helper_28(buf + 524288, 18);
    helper_28(buf + 786432, 18);
    helper_28(buf + 1048576, 18);
    helper_28(buf + 1310720, 18);
    helper_28(buf + 1572864, 18);
    helper_28(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 24) {
    helper_28(buf + 0, 21);
    helper_28(buf + 2097152, 21);
    helper_28(buf + 4194304, 21);
    helper_28(buf + 6291456, 21);
    helper_28(buf + 8388608, 21);
    helper_28(buf + 10485760, 21);
    helper_28(buf + 12582912, 21);
    helper_28(buf + 14680064, 21);
    for (int j = 0; j < 16777216; j += 16777216) {
      for (int k = 0; k < 2097152; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2097152), "r"(buf + j + k + 4194304), "r"(buf + j + k + 6291456), "r"(buf + j + k + 8388608), "r"(buf + j + k + 10485760), "r"(buf + j + k + 12582912), "r"(buf + j + k + 14680064) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 27) {
    helper_28(buf + 0, 24);
    helper_28(buf + 16777216, 24);
    helper_28(buf + 33554432, 24);
    helper_28(buf + 50331648, 24);
    helper_28(buf + 67108864, 24);
    helper_28(buf + 83886080, 24);
    helper_28(buf + 100663296, 24);
    helper_28(buf + 117440512, 24);
    for (int j = 0; j < 134217728; j += 134217728) {
      for (int k = 0; k < 16777216; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16777216), "r"(buf + j + k + 33554432), "r"(buf + j + k + 50331648), "r"(buf + j + k + 67108864), "r"(buf + j + k + 83886080), "r"(buf + j + k + 100663296), "r"(buf + j + k + 117440512) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 28) {
    helper_28(buf + 0, 27);
    helper_28(buf + 134217728, 27);
    for (int j = 0; j < 268435456; j += 268435456) {
      for (int k = 0; k < 134217728; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 134217728) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_29(float *buf, int depth);
void helper_29(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_29(buf + 0, 12);
    helper_29(buf + 4096, 12);
    helper_29(buf + 8192, 12);
    helper_29(buf + 12288, 12);
    helper_29(buf + 16384, 12);
    helper_29(buf + 20480, 12);
    helper_29(buf + 24576, 12);
    helper_29(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_29(buf + 0, 15);
    helper_29(buf + 32768, 15);
    helper_29(buf + 65536, 15);
    helper_29(buf + 98304, 15);
    helper_29(buf + 131072, 15);
    helper_29(buf + 163840, 15);
    helper_29(buf + 196608, 15);
    helper_29(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_29(buf + 0, 18);
    helper_29(buf + 262144, 18);
    helper_29(buf + 524288, 18);
    helper_29(buf + 786432, 18);
    helper_29(buf + 1048576, 18);
    helper_29(buf + 1310720, 18);
    helper_29(buf + 1572864, 18);
    helper_29(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 24) {
    helper_29(buf + 0, 21);
    helper_29(buf + 2097152, 21);
    helper_29(buf + 4194304, 21);
    helper_29(buf + 6291456, 21);
    helper_29(buf + 8388608, 21);
    helper_29(buf + 10485760, 21);
    helper_29(buf + 12582912, 21);
    helper_29(buf + 14680064, 21);
    for (int j = 0; j < 16777216; j += 16777216) {
      for (int k = 0; k < 2097152; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2097152), "r"(buf + j + k + 4194304), "r"(buf + j + k + 6291456), "r"(buf + j + k + 8388608), "r"(buf + j + k + 10485760), "r"(buf + j + k + 12582912), "r"(buf + j + k + 14680064) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 27) {
    helper_29(buf + 0, 24);
    helper_29(buf + 16777216, 24);
    helper_29(buf + 33554432, 24);
    helper_29(buf + 50331648, 24);
    helper_29(buf + 67108864, 24);
    helper_29(buf + 83886080, 24);
    helper_29(buf + 100663296, 24);
    helper_29(buf + 117440512, 24);
    for (int j = 0; j < 134217728; j += 134217728) {
      for (int k = 0; k < 16777216; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16777216), "r"(buf + j + k + 33554432), "r"(buf + j + k + 50331648), "r"(buf + j + k + 67108864), "r"(buf + j + k + 83886080), "r"(buf + j + k + 100663296), "r"(buf + j + k + 117440512) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 29) {
    helper_29(buf + 0, 27);
    helper_29(buf + 134217728, 27);
    helper_29(buf + 268435456, 27);
    helper_29(buf + 402653184, 27);
    for (int j = 0; j < 536870912; j += 536870912) {
      for (int k = 0; k < 134217728; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vmovups %%ymm0, (%0)\n"
          "vmovups %%ymm1, (%1)\n"
          "vmovups %%ymm2, (%2)\n"
          "vmovups %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 134217728), "r"(buf + j + k + 268435456), "r"(buf + j + k + 402653184) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void helper_30(float *buf, int depth);
void helper_30(float *buf, int depth) {
  if (depth == 12) {
    for (int j = 0; j < 4096; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048), "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    helper_30(buf + 0, 12);
    helper_30(buf + 4096, 12);
    helper_30(buf + 8192, 12);
    helper_30(buf + 12288, 12);
    helper_30(buf + 16384, 12);
    helper_30(buf + 20480, 12);
    helper_30(buf + 24576, 12);
    helper_30(buf + 28672, 12);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 4096; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4096), "r"(buf + j + k + 8192), "r"(buf + j + k + 12288), "r"(buf + j + k + 16384), "r"(buf + j + k + 20480), "r"(buf + j + k + 24576), "r"(buf + j + k + 28672) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    helper_30(buf + 0, 15);
    helper_30(buf + 32768, 15);
    helper_30(buf + 65536, 15);
    helper_30(buf + 98304, 15);
    helper_30(buf + 131072, 15);
    helper_30(buf + 163840, 15);
    helper_30(buf + 196608, 15);
    helper_30(buf + 229376, 15);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 32768; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32768), "r"(buf + j + k + 65536), "r"(buf + j + k + 98304), "r"(buf + j + k + 131072), "r"(buf + j + k + 163840), "r"(buf + j + k + 196608), "r"(buf + j + k + 229376) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    helper_30(buf + 0, 18);
    helper_30(buf + 262144, 18);
    helper_30(buf + 524288, 18);
    helper_30(buf + 786432, 18);
    helper_30(buf + 1048576, 18);
    helper_30(buf + 1310720, 18);
    helper_30(buf + 1572864, 18);
    helper_30(buf + 1835008, 18);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 262144; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 262144), "r"(buf + j + k + 524288), "r"(buf + j + k + 786432), "r"(buf + j + k + 1048576), "r"(buf + j + k + 1310720), "r"(buf + j + k + 1572864), "r"(buf + j + k + 1835008) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 24) {
    helper_30(buf + 0, 21);
    helper_30(buf + 2097152, 21);
    helper_30(buf + 4194304, 21);
    helper_30(buf + 6291456, 21);
    helper_30(buf + 8388608, 21);
    helper_30(buf + 10485760, 21);
    helper_30(buf + 12582912, 21);
    helper_30(buf + 14680064, 21);
    for (int j = 0; j < 16777216; j += 16777216) {
      for (int k = 0; k < 2097152; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2097152), "r"(buf + j + k + 4194304), "r"(buf + j + k + 6291456), "r"(buf + j + k + 8388608), "r"(buf + j + k + 10485760), "r"(buf + j + k + 12582912), "r"(buf + j + k + 14680064) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 27) {
    helper_30(buf + 0, 24);
    helper_30(buf + 16777216, 24);
    helper_30(buf + 33554432, 24);
    helper_30(buf + 50331648, 24);
    helper_30(buf + 67108864, 24);
    helper_30(buf + 83886080, 24);
    helper_30(buf + 100663296, 24);
    helper_30(buf + 117440512, 24);
    for (int j = 0; j < 134217728; j += 134217728) {
      for (int k = 0; k < 16777216; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16777216), "r"(buf + j + k + 33554432), "r"(buf + j + k + 50331648), "r"(buf + j + k + 67108864), "r"(buf + j + k + 83886080), "r"(buf + j + k + 100663296), "r"(buf + j + k + 117440512) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 30) {
    helper_30(buf + 0, 27);
    helper_30(buf + 134217728, 27);
    helper_30(buf + 268435456, 27);
    helper_30(buf + 402653184, 27);
    helper_30(buf + 536870912, 27);
    helper_30(buf + 671088640, 27);
    helper_30(buf + 805306368, 27);
    helper_30(buf + 939524096, 27);
    for (int j = 0; j < 1073741824; j += 1073741824) {
      for (int k = 0; k < 134217728; k += 8) {
        __asm__ volatile (
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 134217728), "r"(buf + j + k + 268435456), "r"(buf + j + k + 402653184), "r"(buf + j + k + 536870912), "r"(buf + j + k + 671088640), "r"(buf + j + k + 805306368), "r"(buf + j + k + 939524096) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

inline void double_helper_1(double *buf) {
  for (int j = 0; j < 2; j += 2) {
    for (int k = 0; k < 1; ++k) {
      double u = buf[j + k];
      double v = buf[j + k + 1];
      buf[j + k] = u + v;
      buf[j + k + 1] = u - v;
    }
  }
}

inline void double_helper_2(double *buf);
inline void double_helper_2(double *buf) {
  for (int j = 0; j < 4; j += 4) {
    __asm__ volatile (
      "vmovupd (%0), %%ymm0\n"
      "vpermilpd $0, %%ymm0, %%ymm8\n"
      "vpermilpd $15, %%ymm0, %%ymm9\n"
      "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
      "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
      "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
      "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
      "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
      "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
      "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
      "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
      "vmovupd %%ymm0, (%0)\n"
      :: "r"(buf + j) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
    );
  }
}

inline void double_helper_3(double *buf);
inline void double_helper_3(double *buf) {
  for (int j = 0; j < 8; j += 8) {
    for (int k = 0; k < 4; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vpermilpd $0, %%ymm0, %%ymm8\n"
        "vpermilpd $15, %%ymm0, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilpd $0, %%ymm1, %%ymm8\n"
        "vpermilpd $15, %%ymm1, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 4) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void double_helper_4(double *buf);
inline void double_helper_4(double *buf) {
  for (int j = 0; j < 16; j += 16) {
    for (int k = 0; k < 4; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vpermilpd $0, %%ymm0, %%ymm8\n"
        "vpermilpd $15, %%ymm0, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilpd $0, %%ymm1, %%ymm8\n"
        "vpermilpd $15, %%ymm1, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilpd $0, %%ymm2, %%ymm8\n"
        "vpermilpd $15, %%ymm2, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilpd $0, %%ymm3, %%ymm8\n"
        "vpermilpd $15, %%ymm3, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vmovupd %%ymm0, (%0)\n"
        "vmovupd %%ymm1, (%1)\n"
        "vmovupd %%ymm2, (%2)\n"
        "vmovupd %%ymm3, (%3)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void double_helper_5(double *buf);
inline void double_helper_5(double *buf) {
  for (int j = 0; j < 32; j += 32) {
    for (int k = 0; k < 4; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vpermilpd $0, %%ymm0, %%ymm8\n"
        "vpermilpd $15, %%ymm0, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilpd $0, %%ymm1, %%ymm8\n"
        "vpermilpd $15, %%ymm1, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilpd $0, %%ymm2, %%ymm8\n"
        "vpermilpd $15, %%ymm2, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilpd $0, %%ymm3, %%ymm8\n"
        "vpermilpd $15, %%ymm3, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilpd $0, %%ymm4, %%ymm8\n"
        "vpermilpd $15, %%ymm4, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilpd $0, %%ymm5, %%ymm8\n"
        "vpermilpd $15, %%ymm5, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilpd $0, %%ymm6, %%ymm8\n"
        "vpermilpd $15, %%ymm6, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilpd $0, %%ymm7, %%ymm8\n"
        "vpermilpd $15, %%ymm7, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void double_helper_6(double *buf);
inline void double_helper_6(double *buf) {
  for (int j = 0; j < 64; j += 32) {
    for (int k = 0; k < 4; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vpermilpd $0, %%ymm0, %%ymm8\n"
        "vpermilpd $15, %%ymm0, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilpd $0, %%ymm1, %%ymm8\n"
        "vpermilpd $15, %%ymm1, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilpd $0, %%ymm2, %%ymm8\n"
        "vpermilpd $15, %%ymm2, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilpd $0, %%ymm3, %%ymm8\n"
        "vpermilpd $15, %%ymm3, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilpd $0, %%ymm4, %%ymm8\n"
        "vpermilpd $15, %%ymm4, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilpd $0, %%ymm5, %%ymm8\n"
        "vpermilpd $15, %%ymm5, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilpd $0, %%ymm6, %%ymm8\n"
        "vpermilpd $15, %%ymm6, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilpd $0, %%ymm7, %%ymm8\n"
        "vpermilpd $15, %%ymm7, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 64; j += 64) {
    for (int k = 0; k < 32; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 32) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void double_helper_7(double *buf);
inline void double_helper_7(double *buf) {
  for (int j = 0; j < 128; j += 32) {
    for (int k = 0; k < 4; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vpermilpd $0, %%ymm0, %%ymm8\n"
        "vpermilpd $15, %%ymm0, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilpd $0, %%ymm1, %%ymm8\n"
        "vpermilpd $15, %%ymm1, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilpd $0, %%ymm2, %%ymm8\n"
        "vpermilpd $15, %%ymm2, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilpd $0, %%ymm3, %%ymm8\n"
        "vpermilpd $15, %%ymm3, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilpd $0, %%ymm4, %%ymm8\n"
        "vpermilpd $15, %%ymm4, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilpd $0, %%ymm5, %%ymm8\n"
        "vpermilpd $15, %%ymm5, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilpd $0, %%ymm6, %%ymm8\n"
        "vpermilpd $15, %%ymm6, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilpd $0, %%ymm7, %%ymm8\n"
        "vpermilpd $15, %%ymm7, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 128; j += 128) {
    for (int k = 0; k < 32; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vmovupd %%ymm0, (%0)\n"
        "vmovupd %%ymm1, (%1)\n"
        "vmovupd %%ymm2, (%2)\n"
        "vmovupd %%ymm3, (%3)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void double_helper_8(double *buf);
inline void double_helper_8(double *buf) {
  for (int j = 0; j < 256; j += 32) {
    for (int k = 0; k < 4; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vpermilpd $0, %%ymm0, %%ymm8\n"
        "vpermilpd $15, %%ymm0, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilpd $0, %%ymm1, %%ymm8\n"
        "vpermilpd $15, %%ymm1, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilpd $0, %%ymm2, %%ymm8\n"
        "vpermilpd $15, %%ymm2, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilpd $0, %%ymm3, %%ymm8\n"
        "vpermilpd $15, %%ymm3, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilpd $0, %%ymm4, %%ymm8\n"
        "vpermilpd $15, %%ymm4, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilpd $0, %%ymm5, %%ymm8\n"
        "vpermilpd $15, %%ymm5, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilpd $0, %%ymm6, %%ymm8\n"
        "vpermilpd $15, %%ymm6, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilpd $0, %%ymm7, %%ymm8\n"
        "vpermilpd $15, %%ymm7, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 256; j += 256) {
    for (int k = 0; k < 32; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void double_helper_9(double *buf);
inline void double_helper_9(double *buf) {
  for (int j = 0; j < 512; j += 32) {
    for (int k = 0; k < 4; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vpermilpd $0, %%ymm0, %%ymm8\n"
        "vpermilpd $15, %%ymm0, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilpd $0, %%ymm1, %%ymm8\n"
        "vpermilpd $15, %%ymm1, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilpd $0, %%ymm2, %%ymm8\n"
        "vpermilpd $15, %%ymm2, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilpd $0, %%ymm3, %%ymm8\n"
        "vpermilpd $15, %%ymm3, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilpd $0, %%ymm4, %%ymm8\n"
        "vpermilpd $15, %%ymm4, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilpd $0, %%ymm5, %%ymm8\n"
        "vpermilpd $15, %%ymm5, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilpd $0, %%ymm6, %%ymm8\n"
        "vpermilpd $15, %%ymm6, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilpd $0, %%ymm7, %%ymm8\n"
        "vpermilpd $15, %%ymm7, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 512; j += 256) {
    for (int k = 0; k < 32; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 512; j += 512) {
    for (int k = 0; k < 256; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 256) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void double_helper_10(double *buf);
inline void double_helper_10(double *buf) {
  for (int j = 0; j < 1024; j += 32) {
    for (int k = 0; k < 4; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vpermilpd $0, %%ymm0, %%ymm8\n"
        "vpermilpd $15, %%ymm0, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilpd $0, %%ymm1, %%ymm8\n"
        "vpermilpd $15, %%ymm1, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilpd $0, %%ymm2, %%ymm8\n"
        "vpermilpd $15, %%ymm2, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilpd $0, %%ymm3, %%ymm8\n"
        "vpermilpd $15, %%ymm3, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilpd $0, %%ymm4, %%ymm8\n"
        "vpermilpd $15, %%ymm4, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilpd $0, %%ymm5, %%ymm8\n"
        "vpermilpd $15, %%ymm5, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilpd $0, %%ymm6, %%ymm8\n"
        "vpermilpd $15, %%ymm6, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilpd $0, %%ymm7, %%ymm8\n"
        "vpermilpd $15, %%ymm7, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 1024; j += 256) {
    for (int k = 0; k < 32; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 1024; j += 1024) {
    for (int k = 0; k < 256; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vmovupd %%ymm0, (%0)\n"
        "vmovupd %%ymm1, (%1)\n"
        "vmovupd %%ymm2, (%2)\n"
        "vmovupd %%ymm3, (%3)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

inline void double_helper_11(double *buf);
inline void double_helper_11(double *buf) {
  for (int j = 0; j < 2048; j += 32) {
    for (int k = 0; k < 4; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vpermilpd $0, %%ymm0, %%ymm8\n"
        "vpermilpd $15, %%ymm0, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilpd $0, %%ymm1, %%ymm8\n"
        "vpermilpd $15, %%ymm1, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
        "vpermilpd $0, %%ymm2, %%ymm8\n"
        "vpermilpd $15, %%ymm2, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
        "vpermilpd $0, %%ymm3, %%ymm8\n"
        "vpermilpd $15, %%ymm3, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
        "vpermilpd $0, %%ymm4, %%ymm8\n"
        "vpermilpd $15, %%ymm4, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
        "vpermilpd $0, %%ymm5, %%ymm8\n"
        "vpermilpd $15, %%ymm5, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
        "vpermilpd $0, %%ymm6, %%ymm8\n"
        "vpermilpd $15, %%ymm6, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
        "vpermilpd $0, %%ymm7, %%ymm8\n"
        "vpermilpd $15, %%ymm7, %%ymm9\n"
        "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
        "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
        "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
        "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
        "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
        "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
        "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
        "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
        "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
        "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
        "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
        "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
        "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 2048; j += 256) {
    for (int k = 0; k < 32; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
  for (int j = 0; j < 2048; j += 2048) {
    for (int k = 0; k < 256; k += 4) {
      __asm__ volatile (
        "vmovupd (%0), %%ymm0\n"
        "vmovupd (%1), %%ymm1\n"
        "vmovupd (%2), %%ymm2\n"
        "vmovupd (%3), %%ymm3\n"
        "vmovupd (%4), %%ymm4\n"
        "vmovupd (%5), %%ymm5\n"
        "vmovupd (%6), %%ymm6\n"
        "vmovupd (%7), %%ymm7\n"
        "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
        "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
        "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
        "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
        "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
        "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
        "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
        "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
        "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
        "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
        "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
        "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
        "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
        "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
        "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
        "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
        "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
        "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
        "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
        "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
        "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
        "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
        "vmovupd %%ymm8, (%0)\n"
        "vmovupd %%ymm9, (%1)\n"
        "vmovupd %%ymm10, (%2)\n"
        "vmovupd %%ymm11, (%3)\n"
        "vmovupd %%ymm12, (%4)\n"
        "vmovupd %%ymm13, (%5)\n"
        "vmovupd %%ymm14, (%6)\n"
        "vmovupd %%ymm15, (%7)\n"
        :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
      );
    }
  }
}

void double_helper_12(double *buf, int depth);
void double_helper_12(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 12) {
    double_helper_12(buf + 0, 11);
    double_helper_12(buf + 2048, 11);
    for (int j = 0; j < 4096; j += 4096) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_13(double *buf, int depth);
void double_helper_13(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 13) {
    double_helper_13(buf + 0, 11);
    double_helper_13(buf + 2048, 11);
    double_helper_13(buf + 4096, 11);
    double_helper_13(buf + 6144, 11);
    for (int j = 0; j < 8192; j += 8192) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vmovupd %%ymm0, (%0)\n"
          "vmovupd %%ymm1, (%1)\n"
          "vmovupd %%ymm2, (%2)\n"
          "vmovupd %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_14(double *buf, int depth);
void double_helper_14(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_14(buf + 0, 11);
    double_helper_14(buf + 2048, 11);
    double_helper_14(buf + 4096, 11);
    double_helper_14(buf + 6144, 11);
    double_helper_14(buf + 8192, 11);
    double_helper_14(buf + 10240, 11);
    double_helper_14(buf + 12288, 11);
    double_helper_14(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_15(double *buf, int depth);
void double_helper_15(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_15(buf + 0, 11);
    double_helper_15(buf + 2048, 11);
    double_helper_15(buf + 4096, 11);
    double_helper_15(buf + 6144, 11);
    double_helper_15(buf + 8192, 11);
    double_helper_15(buf + 10240, 11);
    double_helper_15(buf + 12288, 11);
    double_helper_15(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 15) {
    double_helper_15(buf + 0, 14);
    double_helper_15(buf + 16384, 14);
    for (int j = 0; j < 32768; j += 32768) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_16(double *buf, int depth);
void double_helper_16(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_16(buf + 0, 11);
    double_helper_16(buf + 2048, 11);
    double_helper_16(buf + 4096, 11);
    double_helper_16(buf + 6144, 11);
    double_helper_16(buf + 8192, 11);
    double_helper_16(buf + 10240, 11);
    double_helper_16(buf + 12288, 11);
    double_helper_16(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 16) {
    double_helper_16(buf + 0, 14);
    double_helper_16(buf + 16384, 14);
    double_helper_16(buf + 32768, 14);
    double_helper_16(buf + 49152, 14);
    for (int j = 0; j < 65536; j += 65536) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vmovupd %%ymm0, (%0)\n"
          "vmovupd %%ymm1, (%1)\n"
          "vmovupd %%ymm2, (%2)\n"
          "vmovupd %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_17(double *buf, int depth);
void double_helper_17(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_17(buf + 0, 11);
    double_helper_17(buf + 2048, 11);
    double_helper_17(buf + 4096, 11);
    double_helper_17(buf + 6144, 11);
    double_helper_17(buf + 8192, 11);
    double_helper_17(buf + 10240, 11);
    double_helper_17(buf + 12288, 11);
    double_helper_17(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_17(buf + 0, 14);
    double_helper_17(buf + 16384, 14);
    double_helper_17(buf + 32768, 14);
    double_helper_17(buf + 49152, 14);
    double_helper_17(buf + 65536, 14);
    double_helper_17(buf + 81920, 14);
    double_helper_17(buf + 98304, 14);
    double_helper_17(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_18(double *buf, int depth);
void double_helper_18(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_18(buf + 0, 11);
    double_helper_18(buf + 2048, 11);
    double_helper_18(buf + 4096, 11);
    double_helper_18(buf + 6144, 11);
    double_helper_18(buf + 8192, 11);
    double_helper_18(buf + 10240, 11);
    double_helper_18(buf + 12288, 11);
    double_helper_18(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_18(buf + 0, 14);
    double_helper_18(buf + 16384, 14);
    double_helper_18(buf + 32768, 14);
    double_helper_18(buf + 49152, 14);
    double_helper_18(buf + 65536, 14);
    double_helper_18(buf + 81920, 14);
    double_helper_18(buf + 98304, 14);
    double_helper_18(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 18) {
    double_helper_18(buf + 0, 17);
    double_helper_18(buf + 131072, 17);
    for (int j = 0; j < 262144; j += 262144) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_19(double *buf, int depth);
void double_helper_19(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_19(buf + 0, 11);
    double_helper_19(buf + 2048, 11);
    double_helper_19(buf + 4096, 11);
    double_helper_19(buf + 6144, 11);
    double_helper_19(buf + 8192, 11);
    double_helper_19(buf + 10240, 11);
    double_helper_19(buf + 12288, 11);
    double_helper_19(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_19(buf + 0, 14);
    double_helper_19(buf + 16384, 14);
    double_helper_19(buf + 32768, 14);
    double_helper_19(buf + 49152, 14);
    double_helper_19(buf + 65536, 14);
    double_helper_19(buf + 81920, 14);
    double_helper_19(buf + 98304, 14);
    double_helper_19(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 19) {
    double_helper_19(buf + 0, 17);
    double_helper_19(buf + 131072, 17);
    double_helper_19(buf + 262144, 17);
    double_helper_19(buf + 393216, 17);
    for (int j = 0; j < 524288; j += 524288) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vmovupd %%ymm0, (%0)\n"
          "vmovupd %%ymm1, (%1)\n"
          "vmovupd %%ymm2, (%2)\n"
          "vmovupd %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_20(double *buf, int depth);
void double_helper_20(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_20(buf + 0, 11);
    double_helper_20(buf + 2048, 11);
    double_helper_20(buf + 4096, 11);
    double_helper_20(buf + 6144, 11);
    double_helper_20(buf + 8192, 11);
    double_helper_20(buf + 10240, 11);
    double_helper_20(buf + 12288, 11);
    double_helper_20(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_20(buf + 0, 14);
    double_helper_20(buf + 16384, 14);
    double_helper_20(buf + 32768, 14);
    double_helper_20(buf + 49152, 14);
    double_helper_20(buf + 65536, 14);
    double_helper_20(buf + 81920, 14);
    double_helper_20(buf + 98304, 14);
    double_helper_20(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_20(buf + 0, 17);
    double_helper_20(buf + 131072, 17);
    double_helper_20(buf + 262144, 17);
    double_helper_20(buf + 393216, 17);
    double_helper_20(buf + 524288, 17);
    double_helper_20(buf + 655360, 17);
    double_helper_20(buf + 786432, 17);
    double_helper_20(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_21(double *buf, int depth);
void double_helper_21(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_21(buf + 0, 11);
    double_helper_21(buf + 2048, 11);
    double_helper_21(buf + 4096, 11);
    double_helper_21(buf + 6144, 11);
    double_helper_21(buf + 8192, 11);
    double_helper_21(buf + 10240, 11);
    double_helper_21(buf + 12288, 11);
    double_helper_21(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_21(buf + 0, 14);
    double_helper_21(buf + 16384, 14);
    double_helper_21(buf + 32768, 14);
    double_helper_21(buf + 49152, 14);
    double_helper_21(buf + 65536, 14);
    double_helper_21(buf + 81920, 14);
    double_helper_21(buf + 98304, 14);
    double_helper_21(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_21(buf + 0, 17);
    double_helper_21(buf + 131072, 17);
    double_helper_21(buf + 262144, 17);
    double_helper_21(buf + 393216, 17);
    double_helper_21(buf + 524288, 17);
    double_helper_21(buf + 655360, 17);
    double_helper_21(buf + 786432, 17);
    double_helper_21(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 21) {
    double_helper_21(buf + 0, 20);
    double_helper_21(buf + 1048576, 20);
    for (int j = 0; j < 2097152; j += 2097152) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_22(double *buf, int depth);
void double_helper_22(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_22(buf + 0, 11);
    double_helper_22(buf + 2048, 11);
    double_helper_22(buf + 4096, 11);
    double_helper_22(buf + 6144, 11);
    double_helper_22(buf + 8192, 11);
    double_helper_22(buf + 10240, 11);
    double_helper_22(buf + 12288, 11);
    double_helper_22(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_22(buf + 0, 14);
    double_helper_22(buf + 16384, 14);
    double_helper_22(buf + 32768, 14);
    double_helper_22(buf + 49152, 14);
    double_helper_22(buf + 65536, 14);
    double_helper_22(buf + 81920, 14);
    double_helper_22(buf + 98304, 14);
    double_helper_22(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_22(buf + 0, 17);
    double_helper_22(buf + 131072, 17);
    double_helper_22(buf + 262144, 17);
    double_helper_22(buf + 393216, 17);
    double_helper_22(buf + 524288, 17);
    double_helper_22(buf + 655360, 17);
    double_helper_22(buf + 786432, 17);
    double_helper_22(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 22) {
    double_helper_22(buf + 0, 20);
    double_helper_22(buf + 1048576, 20);
    double_helper_22(buf + 2097152, 20);
    double_helper_22(buf + 3145728, 20);
    for (int j = 0; j < 4194304; j += 4194304) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vmovupd %%ymm0, (%0)\n"
          "vmovupd %%ymm1, (%1)\n"
          "vmovupd %%ymm2, (%2)\n"
          "vmovupd %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576), "r"(buf + j + k + 2097152), "r"(buf + j + k + 3145728) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_23(double *buf, int depth);
void double_helper_23(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_23(buf + 0, 11);
    double_helper_23(buf + 2048, 11);
    double_helper_23(buf + 4096, 11);
    double_helper_23(buf + 6144, 11);
    double_helper_23(buf + 8192, 11);
    double_helper_23(buf + 10240, 11);
    double_helper_23(buf + 12288, 11);
    double_helper_23(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_23(buf + 0, 14);
    double_helper_23(buf + 16384, 14);
    double_helper_23(buf + 32768, 14);
    double_helper_23(buf + 49152, 14);
    double_helper_23(buf + 65536, 14);
    double_helper_23(buf + 81920, 14);
    double_helper_23(buf + 98304, 14);
    double_helper_23(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_23(buf + 0, 17);
    double_helper_23(buf + 131072, 17);
    double_helper_23(buf + 262144, 17);
    double_helper_23(buf + 393216, 17);
    double_helper_23(buf + 524288, 17);
    double_helper_23(buf + 655360, 17);
    double_helper_23(buf + 786432, 17);
    double_helper_23(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 23) {
    double_helper_23(buf + 0, 20);
    double_helper_23(buf + 1048576, 20);
    double_helper_23(buf + 2097152, 20);
    double_helper_23(buf + 3145728, 20);
    double_helper_23(buf + 4194304, 20);
    double_helper_23(buf + 5242880, 20);
    double_helper_23(buf + 6291456, 20);
    double_helper_23(buf + 7340032, 20);
    for (int j = 0; j < 8388608; j += 8388608) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576), "r"(buf + j + k + 2097152), "r"(buf + j + k + 3145728), "r"(buf + j + k + 4194304), "r"(buf + j + k + 5242880), "r"(buf + j + k + 6291456), "r"(buf + j + k + 7340032) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_24(double *buf, int depth);
void double_helper_24(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_24(buf + 0, 11);
    double_helper_24(buf + 2048, 11);
    double_helper_24(buf + 4096, 11);
    double_helper_24(buf + 6144, 11);
    double_helper_24(buf + 8192, 11);
    double_helper_24(buf + 10240, 11);
    double_helper_24(buf + 12288, 11);
    double_helper_24(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_24(buf + 0, 14);
    double_helper_24(buf + 16384, 14);
    double_helper_24(buf + 32768, 14);
    double_helper_24(buf + 49152, 14);
    double_helper_24(buf + 65536, 14);
    double_helper_24(buf + 81920, 14);
    double_helper_24(buf + 98304, 14);
    double_helper_24(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_24(buf + 0, 17);
    double_helper_24(buf + 131072, 17);
    double_helper_24(buf + 262144, 17);
    double_helper_24(buf + 393216, 17);
    double_helper_24(buf + 524288, 17);
    double_helper_24(buf + 655360, 17);
    double_helper_24(buf + 786432, 17);
    double_helper_24(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 23) {
    double_helper_24(buf + 0, 20);
    double_helper_24(buf + 1048576, 20);
    double_helper_24(buf + 2097152, 20);
    double_helper_24(buf + 3145728, 20);
    double_helper_24(buf + 4194304, 20);
    double_helper_24(buf + 5242880, 20);
    double_helper_24(buf + 6291456, 20);
    double_helper_24(buf + 7340032, 20);
    for (int j = 0; j < 8388608; j += 8388608) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576), "r"(buf + j + k + 2097152), "r"(buf + j + k + 3145728), "r"(buf + j + k + 4194304), "r"(buf + j + k + 5242880), "r"(buf + j + k + 6291456), "r"(buf + j + k + 7340032) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 24) {
    double_helper_24(buf + 0, 23);
    double_helper_24(buf + 8388608, 23);
    for (int j = 0; j < 16777216; j += 16777216) {
      for (int k = 0; k < 8388608; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8388608) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_25(double *buf, int depth);
void double_helper_25(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_25(buf + 0, 11);
    double_helper_25(buf + 2048, 11);
    double_helper_25(buf + 4096, 11);
    double_helper_25(buf + 6144, 11);
    double_helper_25(buf + 8192, 11);
    double_helper_25(buf + 10240, 11);
    double_helper_25(buf + 12288, 11);
    double_helper_25(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_25(buf + 0, 14);
    double_helper_25(buf + 16384, 14);
    double_helper_25(buf + 32768, 14);
    double_helper_25(buf + 49152, 14);
    double_helper_25(buf + 65536, 14);
    double_helper_25(buf + 81920, 14);
    double_helper_25(buf + 98304, 14);
    double_helper_25(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_25(buf + 0, 17);
    double_helper_25(buf + 131072, 17);
    double_helper_25(buf + 262144, 17);
    double_helper_25(buf + 393216, 17);
    double_helper_25(buf + 524288, 17);
    double_helper_25(buf + 655360, 17);
    double_helper_25(buf + 786432, 17);
    double_helper_25(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 23) {
    double_helper_25(buf + 0, 20);
    double_helper_25(buf + 1048576, 20);
    double_helper_25(buf + 2097152, 20);
    double_helper_25(buf + 3145728, 20);
    double_helper_25(buf + 4194304, 20);
    double_helper_25(buf + 5242880, 20);
    double_helper_25(buf + 6291456, 20);
    double_helper_25(buf + 7340032, 20);
    for (int j = 0; j < 8388608; j += 8388608) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576), "r"(buf + j + k + 2097152), "r"(buf + j + k + 3145728), "r"(buf + j + k + 4194304), "r"(buf + j + k + 5242880), "r"(buf + j + k + 6291456), "r"(buf + j + k + 7340032) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 25) {
    double_helper_25(buf + 0, 23);
    double_helper_25(buf + 8388608, 23);
    double_helper_25(buf + 16777216, 23);
    double_helper_25(buf + 25165824, 23);
    for (int j = 0; j < 33554432; j += 33554432) {
      for (int k = 0; k < 8388608; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vmovupd %%ymm0, (%0)\n"
          "vmovupd %%ymm1, (%1)\n"
          "vmovupd %%ymm2, (%2)\n"
          "vmovupd %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8388608), "r"(buf + j + k + 16777216), "r"(buf + j + k + 25165824) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_26(double *buf, int depth);
void double_helper_26(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_26(buf + 0, 11);
    double_helper_26(buf + 2048, 11);
    double_helper_26(buf + 4096, 11);
    double_helper_26(buf + 6144, 11);
    double_helper_26(buf + 8192, 11);
    double_helper_26(buf + 10240, 11);
    double_helper_26(buf + 12288, 11);
    double_helper_26(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_26(buf + 0, 14);
    double_helper_26(buf + 16384, 14);
    double_helper_26(buf + 32768, 14);
    double_helper_26(buf + 49152, 14);
    double_helper_26(buf + 65536, 14);
    double_helper_26(buf + 81920, 14);
    double_helper_26(buf + 98304, 14);
    double_helper_26(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_26(buf + 0, 17);
    double_helper_26(buf + 131072, 17);
    double_helper_26(buf + 262144, 17);
    double_helper_26(buf + 393216, 17);
    double_helper_26(buf + 524288, 17);
    double_helper_26(buf + 655360, 17);
    double_helper_26(buf + 786432, 17);
    double_helper_26(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 23) {
    double_helper_26(buf + 0, 20);
    double_helper_26(buf + 1048576, 20);
    double_helper_26(buf + 2097152, 20);
    double_helper_26(buf + 3145728, 20);
    double_helper_26(buf + 4194304, 20);
    double_helper_26(buf + 5242880, 20);
    double_helper_26(buf + 6291456, 20);
    double_helper_26(buf + 7340032, 20);
    for (int j = 0; j < 8388608; j += 8388608) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576), "r"(buf + j + k + 2097152), "r"(buf + j + k + 3145728), "r"(buf + j + k + 4194304), "r"(buf + j + k + 5242880), "r"(buf + j + k + 6291456), "r"(buf + j + k + 7340032) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 26) {
    double_helper_26(buf + 0, 23);
    double_helper_26(buf + 8388608, 23);
    double_helper_26(buf + 16777216, 23);
    double_helper_26(buf + 25165824, 23);
    double_helper_26(buf + 33554432, 23);
    double_helper_26(buf + 41943040, 23);
    double_helper_26(buf + 50331648, 23);
    double_helper_26(buf + 58720256, 23);
    for (int j = 0; j < 67108864; j += 67108864) {
      for (int k = 0; k < 8388608; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8388608), "r"(buf + j + k + 16777216), "r"(buf + j + k + 25165824), "r"(buf + j + k + 33554432), "r"(buf + j + k + 41943040), "r"(buf + j + k + 50331648), "r"(buf + j + k + 58720256) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_27(double *buf, int depth);
void double_helper_27(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_27(buf + 0, 11);
    double_helper_27(buf + 2048, 11);
    double_helper_27(buf + 4096, 11);
    double_helper_27(buf + 6144, 11);
    double_helper_27(buf + 8192, 11);
    double_helper_27(buf + 10240, 11);
    double_helper_27(buf + 12288, 11);
    double_helper_27(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_27(buf + 0, 14);
    double_helper_27(buf + 16384, 14);
    double_helper_27(buf + 32768, 14);
    double_helper_27(buf + 49152, 14);
    double_helper_27(buf + 65536, 14);
    double_helper_27(buf + 81920, 14);
    double_helper_27(buf + 98304, 14);
    double_helper_27(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_27(buf + 0, 17);
    double_helper_27(buf + 131072, 17);
    double_helper_27(buf + 262144, 17);
    double_helper_27(buf + 393216, 17);
    double_helper_27(buf + 524288, 17);
    double_helper_27(buf + 655360, 17);
    double_helper_27(buf + 786432, 17);
    double_helper_27(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 23) {
    double_helper_27(buf + 0, 20);
    double_helper_27(buf + 1048576, 20);
    double_helper_27(buf + 2097152, 20);
    double_helper_27(buf + 3145728, 20);
    double_helper_27(buf + 4194304, 20);
    double_helper_27(buf + 5242880, 20);
    double_helper_27(buf + 6291456, 20);
    double_helper_27(buf + 7340032, 20);
    for (int j = 0; j < 8388608; j += 8388608) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576), "r"(buf + j + k + 2097152), "r"(buf + j + k + 3145728), "r"(buf + j + k + 4194304), "r"(buf + j + k + 5242880), "r"(buf + j + k + 6291456), "r"(buf + j + k + 7340032) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 26) {
    double_helper_27(buf + 0, 23);
    double_helper_27(buf + 8388608, 23);
    double_helper_27(buf + 16777216, 23);
    double_helper_27(buf + 25165824, 23);
    double_helper_27(buf + 33554432, 23);
    double_helper_27(buf + 41943040, 23);
    double_helper_27(buf + 50331648, 23);
    double_helper_27(buf + 58720256, 23);
    for (int j = 0; j < 67108864; j += 67108864) {
      for (int k = 0; k < 8388608; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8388608), "r"(buf + j + k + 16777216), "r"(buf + j + k + 25165824), "r"(buf + j + k + 33554432), "r"(buf + j + k + 41943040), "r"(buf + j + k + 50331648), "r"(buf + j + k + 58720256) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 27) {
    double_helper_27(buf + 0, 26);
    double_helper_27(buf + 67108864, 26);
    for (int j = 0; j < 134217728; j += 134217728) {
      for (int k = 0; k < 67108864; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 67108864) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_28(double *buf, int depth);
void double_helper_28(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_28(buf + 0, 11);
    double_helper_28(buf + 2048, 11);
    double_helper_28(buf + 4096, 11);
    double_helper_28(buf + 6144, 11);
    double_helper_28(buf + 8192, 11);
    double_helper_28(buf + 10240, 11);
    double_helper_28(buf + 12288, 11);
    double_helper_28(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_28(buf + 0, 14);
    double_helper_28(buf + 16384, 14);
    double_helper_28(buf + 32768, 14);
    double_helper_28(buf + 49152, 14);
    double_helper_28(buf + 65536, 14);
    double_helper_28(buf + 81920, 14);
    double_helper_28(buf + 98304, 14);
    double_helper_28(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_28(buf + 0, 17);
    double_helper_28(buf + 131072, 17);
    double_helper_28(buf + 262144, 17);
    double_helper_28(buf + 393216, 17);
    double_helper_28(buf + 524288, 17);
    double_helper_28(buf + 655360, 17);
    double_helper_28(buf + 786432, 17);
    double_helper_28(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 23) {
    double_helper_28(buf + 0, 20);
    double_helper_28(buf + 1048576, 20);
    double_helper_28(buf + 2097152, 20);
    double_helper_28(buf + 3145728, 20);
    double_helper_28(buf + 4194304, 20);
    double_helper_28(buf + 5242880, 20);
    double_helper_28(buf + 6291456, 20);
    double_helper_28(buf + 7340032, 20);
    for (int j = 0; j < 8388608; j += 8388608) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576), "r"(buf + j + k + 2097152), "r"(buf + j + k + 3145728), "r"(buf + j + k + 4194304), "r"(buf + j + k + 5242880), "r"(buf + j + k + 6291456), "r"(buf + j + k + 7340032) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 26) {
    double_helper_28(buf + 0, 23);
    double_helper_28(buf + 8388608, 23);
    double_helper_28(buf + 16777216, 23);
    double_helper_28(buf + 25165824, 23);
    double_helper_28(buf + 33554432, 23);
    double_helper_28(buf + 41943040, 23);
    double_helper_28(buf + 50331648, 23);
    double_helper_28(buf + 58720256, 23);
    for (int j = 0; j < 67108864; j += 67108864) {
      for (int k = 0; k < 8388608; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8388608), "r"(buf + j + k + 16777216), "r"(buf + j + k + 25165824), "r"(buf + j + k + 33554432), "r"(buf + j + k + 41943040), "r"(buf + j + k + 50331648), "r"(buf + j + k + 58720256) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 28) {
    double_helper_28(buf + 0, 26);
    double_helper_28(buf + 67108864, 26);
    double_helper_28(buf + 134217728, 26);
    double_helper_28(buf + 201326592, 26);
    for (int j = 0; j < 268435456; j += 268435456) {
      for (int k = 0; k < 67108864; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vmovupd %%ymm0, (%0)\n"
          "vmovupd %%ymm1, (%1)\n"
          "vmovupd %%ymm2, (%2)\n"
          "vmovupd %%ymm3, (%3)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 67108864), "r"(buf + j + k + 134217728), "r"(buf + j + k + 201326592) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_29(double *buf, int depth);
void double_helper_29(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_29(buf + 0, 11);
    double_helper_29(buf + 2048, 11);
    double_helper_29(buf + 4096, 11);
    double_helper_29(buf + 6144, 11);
    double_helper_29(buf + 8192, 11);
    double_helper_29(buf + 10240, 11);
    double_helper_29(buf + 12288, 11);
    double_helper_29(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_29(buf + 0, 14);
    double_helper_29(buf + 16384, 14);
    double_helper_29(buf + 32768, 14);
    double_helper_29(buf + 49152, 14);
    double_helper_29(buf + 65536, 14);
    double_helper_29(buf + 81920, 14);
    double_helper_29(buf + 98304, 14);
    double_helper_29(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_29(buf + 0, 17);
    double_helper_29(buf + 131072, 17);
    double_helper_29(buf + 262144, 17);
    double_helper_29(buf + 393216, 17);
    double_helper_29(buf + 524288, 17);
    double_helper_29(buf + 655360, 17);
    double_helper_29(buf + 786432, 17);
    double_helper_29(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 23) {
    double_helper_29(buf + 0, 20);
    double_helper_29(buf + 1048576, 20);
    double_helper_29(buf + 2097152, 20);
    double_helper_29(buf + 3145728, 20);
    double_helper_29(buf + 4194304, 20);
    double_helper_29(buf + 5242880, 20);
    double_helper_29(buf + 6291456, 20);
    double_helper_29(buf + 7340032, 20);
    for (int j = 0; j < 8388608; j += 8388608) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576), "r"(buf + j + k + 2097152), "r"(buf + j + k + 3145728), "r"(buf + j + k + 4194304), "r"(buf + j + k + 5242880), "r"(buf + j + k + 6291456), "r"(buf + j + k + 7340032) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 26) {
    double_helper_29(buf + 0, 23);
    double_helper_29(buf + 8388608, 23);
    double_helper_29(buf + 16777216, 23);
    double_helper_29(buf + 25165824, 23);
    double_helper_29(buf + 33554432, 23);
    double_helper_29(buf + 41943040, 23);
    double_helper_29(buf + 50331648, 23);
    double_helper_29(buf + 58720256, 23);
    for (int j = 0; j < 67108864; j += 67108864) {
      for (int k = 0; k < 8388608; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8388608), "r"(buf + j + k + 16777216), "r"(buf + j + k + 25165824), "r"(buf + j + k + 33554432), "r"(buf + j + k + 41943040), "r"(buf + j + k + 50331648), "r"(buf + j + k + 58720256) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 29) {
    double_helper_29(buf + 0, 26);
    double_helper_29(buf + 67108864, 26);
    double_helper_29(buf + 134217728, 26);
    double_helper_29(buf + 201326592, 26);
    double_helper_29(buf + 268435456, 26);
    double_helper_29(buf + 335544320, 26);
    double_helper_29(buf + 402653184, 26);
    double_helper_29(buf + 469762048, 26);
    for (int j = 0; j < 536870912; j += 536870912) {
      for (int k = 0; k < 67108864; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 67108864), "r"(buf + j + k + 134217728), "r"(buf + j + k + 201326592), "r"(buf + j + k + 268435456), "r"(buf + j + k + 335544320), "r"(buf + j + k + 402653184), "r"(buf + j + k + 469762048) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void double_helper_30(double *buf, int depth);
void double_helper_30(double *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 32) {
      for (int k = 0; k < 4; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vpermilpd $0, %%ymm0, %%ymm8\n"
          "vpermilpd $15, %%ymm0, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilpd $0, %%ymm1, %%ymm8\n"
          "vpermilpd $15, %%ymm1, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilpd $0, %%ymm2, %%ymm8\n"
          "vpermilpd $15, %%ymm2, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilpd $0, %%ymm3, %%ymm8\n"
          "vpermilpd $15, %%ymm3, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilpd $0, %%ymm4, %%ymm8\n"
          "vpermilpd $15, %%ymm4, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilpd $0, %%ymm5, %%ymm8\n"
          "vpermilpd $15, %%ymm5, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilpd $0, %%ymm6, %%ymm8\n"
          "vpermilpd $15, %%ymm6, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilpd $0, %%ymm7, %%ymm8\n"
          "vpermilpd $15, %%ymm7, %%ymm9\n"
          "vxorpd %%ymm10, %%ymm10, %%ymm10\n"
          "vsubpd %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubpd %%ymm11, %%ymm8, %%ymm7\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm0, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm0, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm0\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm1, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm1, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm1\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm2, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm2\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm3, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm3, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm3\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm4, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm4, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm4\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm5, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm5\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm6, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm6, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm6\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm8\n"
          "vxorpd %%ymm9, %%ymm9, %%ymm9\n"
          "vsubpd %%ymm7, %%ymm9, %%ymm10\n"
          "vperm2f128 $49, %%ymm10, %%ymm7, %%ymm11\n"
          "vaddpd %%ymm11, %%ymm8, %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 4), "r"(buf + j + k + 8), "r"(buf + j + k + 12), "r"(buf + j + k + 16), "r"(buf + j + k + 20), "r"(buf + j + k + 24), "r"(buf + j + k + 28) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 256) {
      for (int k = 0; k < 32; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 32), "r"(buf + j + k + 64), "r"(buf + j + k + 96), "r"(buf + j + k + 128), "r"(buf + j + k + 160), "r"(buf + j + k + 192), "r"(buf + j + k + 224) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 256; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 256), "r"(buf + j + k + 512), "r"(buf + j + k + 768), "r"(buf + j + k + 1024), "r"(buf + j + k + 1280), "r"(buf + j + k + 1536), "r"(buf + j + k + 1792) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 14) {
    double_helper_30(buf + 0, 11);
    double_helper_30(buf + 2048, 11);
    double_helper_30(buf + 4096, 11);
    double_helper_30(buf + 6144, 11);
    double_helper_30(buf + 8192, 11);
    double_helper_30(buf + 10240, 11);
    double_helper_30(buf + 12288, 11);
    double_helper_30(buf + 14336, 11);
    for (int j = 0; j < 16384; j += 16384) {
      for (int k = 0; k < 2048; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 2048), "r"(buf + j + k + 4096), "r"(buf + j + k + 6144), "r"(buf + j + k + 8192), "r"(buf + j + k + 10240), "r"(buf + j + k + 12288), "r"(buf + j + k + 14336) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 17) {
    double_helper_30(buf + 0, 14);
    double_helper_30(buf + 16384, 14);
    double_helper_30(buf + 32768, 14);
    double_helper_30(buf + 49152, 14);
    double_helper_30(buf + 65536, 14);
    double_helper_30(buf + 81920, 14);
    double_helper_30(buf + 98304, 14);
    double_helper_30(buf + 114688, 14);
    for (int j = 0; j < 131072; j += 131072) {
      for (int k = 0; k < 16384; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 16384), "r"(buf + j + k + 32768), "r"(buf + j + k + 49152), "r"(buf + j + k + 65536), "r"(buf + j + k + 81920), "r"(buf + j + k + 98304), "r"(buf + j + k + 114688) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 20) {
    double_helper_30(buf + 0, 17);
    double_helper_30(buf + 131072, 17);
    double_helper_30(buf + 262144, 17);
    double_helper_30(buf + 393216, 17);
    double_helper_30(buf + 524288, 17);
    double_helper_30(buf + 655360, 17);
    double_helper_30(buf + 786432, 17);
    double_helper_30(buf + 917504, 17);
    for (int j = 0; j < 1048576; j += 1048576) {
      for (int k = 0; k < 131072; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 131072), "r"(buf + j + k + 262144), "r"(buf + j + k + 393216), "r"(buf + j + k + 524288), "r"(buf + j + k + 655360), "r"(buf + j + k + 786432), "r"(buf + j + k + 917504) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 23) {
    double_helper_30(buf + 0, 20);
    double_helper_30(buf + 1048576, 20);
    double_helper_30(buf + 2097152, 20);
    double_helper_30(buf + 3145728, 20);
    double_helper_30(buf + 4194304, 20);
    double_helper_30(buf + 5242880, 20);
    double_helper_30(buf + 6291456, 20);
    double_helper_30(buf + 7340032, 20);
    for (int j = 0; j < 8388608; j += 8388608) {
      for (int k = 0; k < 1048576; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 1048576), "r"(buf + j + k + 2097152), "r"(buf + j + k + 3145728), "r"(buf + j + k + 4194304), "r"(buf + j + k + 5242880), "r"(buf + j + k + 6291456), "r"(buf + j + k + 7340032) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 26) {
    double_helper_30(buf + 0, 23);
    double_helper_30(buf + 8388608, 23);
    double_helper_30(buf + 16777216, 23);
    double_helper_30(buf + 25165824, 23);
    double_helper_30(buf + 33554432, 23);
    double_helper_30(buf + 41943040, 23);
    double_helper_30(buf + 50331648, 23);
    double_helper_30(buf + 58720256, 23);
    for (int j = 0; j < 67108864; j += 67108864) {
      for (int k = 0; k < 8388608; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 8388608), "r"(buf + j + k + 16777216), "r"(buf + j + k + 25165824), "r"(buf + j + k + 33554432), "r"(buf + j + k + 41943040), "r"(buf + j + k + 50331648), "r"(buf + j + k + 58720256) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 29) {
    double_helper_30(buf + 0, 26);
    double_helper_30(buf + 67108864, 26);
    double_helper_30(buf + 134217728, 26);
    double_helper_30(buf + 201326592, 26);
    double_helper_30(buf + 268435456, 26);
    double_helper_30(buf + 335544320, 26);
    double_helper_30(buf + 402653184, 26);
    double_helper_30(buf + 469762048, 26);
    for (int j = 0; j < 536870912; j += 536870912) {
      for (int k = 0; k < 67108864; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vmovupd (%2), %%ymm2\n"
          "vmovupd (%3), %%ymm3\n"
          "vmovupd (%4), %%ymm4\n"
          "vmovupd (%5), %%ymm5\n"
          "vmovupd (%6), %%ymm6\n"
          "vmovupd (%7), %%ymm7\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vaddpd %%ymm3, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm3, %%ymm2, %%ymm11\n"
          "vaddpd %%ymm5, %%ymm4, %%ymm12\n"
          "vsubpd %%ymm5, %%ymm4, %%ymm13\n"
          "vaddpd %%ymm7, %%ymm6, %%ymm14\n"
          "vsubpd %%ymm7, %%ymm6, %%ymm15\n"
          "vaddpd %%ymm10, %%ymm8, %%ymm0\n"
          "vsubpd %%ymm10, %%ymm8, %%ymm2\n"
          "vaddpd %%ymm11, %%ymm9, %%ymm1\n"
          "vsubpd %%ymm11, %%ymm9, %%ymm3\n"
          "vaddpd %%ymm14, %%ymm12, %%ymm4\n"
          "vsubpd %%ymm14, %%ymm12, %%ymm6\n"
          "vaddpd %%ymm15, %%ymm13, %%ymm5\n"
          "vsubpd %%ymm15, %%ymm13, %%ymm7\n"
          "vaddpd %%ymm4, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm4, %%ymm0, %%ymm12\n"
          "vaddpd %%ymm5, %%ymm1, %%ymm9\n"
          "vsubpd %%ymm5, %%ymm1, %%ymm13\n"
          "vaddpd %%ymm6, %%ymm2, %%ymm10\n"
          "vsubpd %%ymm6, %%ymm2, %%ymm14\n"
          "vaddpd %%ymm7, %%ymm3, %%ymm11\n"
          "vsubpd %%ymm7, %%ymm3, %%ymm15\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          "vmovupd %%ymm10, (%2)\n"
          "vmovupd %%ymm11, (%3)\n"
          "vmovupd %%ymm12, (%4)\n"
          "vmovupd %%ymm13, (%5)\n"
          "vmovupd %%ymm14, (%6)\n"
          "vmovupd %%ymm15, (%7)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 67108864), "r"(buf + j + k + 134217728), "r"(buf + j + k + 201326592), "r"(buf + j + k + 268435456), "r"(buf + j + k + 335544320), "r"(buf + j + k + 402653184), "r"(buf + j + k + 469762048) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
  if (depth == 30) {
    double_helper_30(buf + 0, 29);
    double_helper_30(buf + 536870912, 29);
    for (int j = 0; j < 1073741824; j += 1073741824) {
      for (int k = 0; k < 536870912; k += 4) {
        __asm__ volatile (
          "vmovupd (%0), %%ymm0\n"
          "vmovupd (%1), %%ymm1\n"
          "vaddpd %%ymm1, %%ymm0, %%ymm8\n"
          "vsubpd %%ymm1, %%ymm0, %%ymm9\n"
          "vmovupd %%ymm8, (%0)\n"
          "vmovupd %%ymm9, (%1)\n"
          :: "r"(buf + j + k + 0), "r"(buf + j + k + 536870912) : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory"
        );
      }
    }
    return;
  }
}

void fht_float(float *buf, int log_n) {
  if (log_n == 0) {
  }
  else if (log_n == 1) {
    helper_1(buf);
  }
  else if (log_n == 2) {
    helper_2(buf);
  }
  else if (log_n == 3) {
    helper_3(buf);
  }
  else if (log_n == 4) {
    helper_4(buf);
  }
  else if (log_n == 5) {
    helper_5(buf);
  }
  else if (log_n == 6) {
    helper_6(buf);
  }
  else if (log_n == 7) {
    helper_7(buf);
  }
  else if (log_n == 8) {
    helper_8(buf);
  }
  else if (log_n == 9) {
    helper_9(buf);
  }
  else if (log_n == 10) {
    helper_10(buf);
  }
  else if (log_n == 11) {
    helper_11(buf);
  }
  else if (log_n == 12) {
    helper_12(buf);
  }
  else if (log_n == 13) {
    helper_13(buf, 13);
  }
  else if (log_n == 14) {
    helper_14(buf, 14);
  }
  else if (log_n == 15) {
    helper_15(buf, 15);
  }
  else if (log_n == 16) {
    helper_16(buf, 16);
  }
  else if (log_n == 17) {
    helper_17(buf, 17);
  }
  else if (log_n == 18) {
    helper_18(buf, 18);
  }
  else if (log_n == 19) {
    helper_19(buf, 19);
  }
  else if (log_n == 20) {
    helper_20(buf, 20);
  }
  else if (log_n == 21) {
    helper_21(buf, 21);
  }
  else if (log_n == 22) {
    helper_22(buf, 22);
  }
  else if (log_n == 23) {
    helper_23(buf, 23);
  }
  else if (log_n == 24) {
    helper_24(buf, 24);
  }
  else if (log_n == 25) {
    helper_25(buf, 25);
  }
  else if (log_n == 26) {
    helper_26(buf, 26);
  }
  else if (log_n == 27) {
    helper_27(buf, 27);
  }
  else if (log_n == 28) {
    helper_28(buf, 28);
  }
  else if (log_n == 29) {
    helper_29(buf, 29);
  }
  else if (log_n == 30) {
    helper_30(buf, 30);
  }
  else {
  }
}
void fht_double(double *buf, int log_n) {
  if (log_n == 0) {
  }
  else if (log_n == 1) {
    double_helper_1(buf);
  }
  else if (log_n == 2) {
    double_helper_2(buf);
  }
  else if (log_n == 3) {
    double_helper_3(buf);
  }
  else if (log_n == 4) {
    double_helper_4(buf);
  }
  else if (log_n == 5) {
    double_helper_5(buf);
  }
  else if (log_n == 6) {
    double_helper_6(buf);
  }
  else if (log_n == 7) {
    double_helper_7(buf);
  }
  else if (log_n == 8) {
    double_helper_8(buf);
  }
  else if (log_n == 9) {
    double_helper_9(buf);
  }
  else if (log_n == 10) {
    double_helper_10(buf);
  }
  else if (log_n == 11) {
    double_helper_11(buf);
  }
  else if (log_n == 12) {
    double_helper_12(buf, 12);
  }
  else if (log_n == 13) {
    double_helper_13(buf, 13);
  }
  else if (log_n == 14) {
    double_helper_14(buf, 14);
  }
  else if (log_n == 15) {
    double_helper_15(buf, 15);
  }
  else if (log_n == 16) {
    double_helper_16(buf, 16);
  }
  else if (log_n == 17) {
    double_helper_17(buf, 17);
  }
  else if (log_n == 18) {
    double_helper_18(buf, 18);
  }
  else if (log_n == 19) {
    double_helper_19(buf, 19);
  }
  else if (log_n == 20) {
    double_helper_20(buf, 20);
  }
  else if (log_n == 21) {
    double_helper_21(buf, 21);
  }
  else if (log_n == 22) {
    double_helper_22(buf, 22);
  }
  else if (log_n == 23) {
    double_helper_23(buf, 23);
  }
  else if (log_n == 24) {
    double_helper_24(buf, 24);
  }
  else if (log_n == 25) {
    double_helper_25(buf, 25);
  }
  else if (log_n == 26) {
    double_helper_26(buf, 26);
  }
  else if (log_n == 27) {
    double_helper_27(buf, 27);
  }
  else if (log_n == 28) {
    double_helper_28(buf, 28);
  }
  else if (log_n == 29) {
    double_helper_29(buf, 29);
  }
  else if (log_n == 30) {
    double_helper_30(buf, 30);
  }
  else {
  }
}

